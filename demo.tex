\documentclass[10pt]{beamer}
\usepackage{amsmath}
\usepackage{mathdots}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetheme[progressbar=frametitle]{metropolis}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{pgfplots}
\usepackage{amsmath}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepgfplotslibrary{dateplot}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{A Fault Detection method based on SVM technique with $T^2$-Statistic and Riemannian metric}
%\subtitle{going beyond IF and Scopus index (v2.0)}
\author{Zhongcheng Dai  \\
 Supervisor: \\
 Prof. Dr. Ing. Steven Ding\\
Dr. Ing. Chris Louen 
}
 \date{30 November 2019}
\institute{Automatic Control and Complex Systems}
\titlegraphic{\hfill\includegraphics[height=1cm]{logo_ITB}}
\begin{document}
\maketitle
\begin{frame}{contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}
\section{Introduction}
\begin{frame}{Background}
\metroset{block=fill}
    \begin{exampleblock}{Traditional methods}
	\begin{itemize}
    \item $T^2$-statistic
    \item Q-statistic
    \item Kullback-Leibler divergence
    \item The Wishart distribution-based methods
    \item \dots \dots
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Background}
 A brief summary of the steps:
      \begin{itemize}
      \item \textcolor{blue}{S1}: collect sufficient fault-free data
      \item \textcolor{blue}{S2}: offline training to determine the threshold
      \item \textcolor{blue}{S3}: online Monitoring  % use the threshold to check the new data
 	 \end{itemize}  
 Detection logic:
 \begin{equation}\nonumber
 \begin{aligned}
     &J > J_{th} \quad faulty \\
     &J < J_{th} \quad fault-free \\
     \end{aligned}
 \end{equation}
\end{frame}
\begin{frame}{Background}
 \begin{exampleblock}{Merits of the traditional methods}
	\begin{itemize}
    \item The detection performance for additive faults is satisfactory
    \item less computational cost(threshold and Online monitoring)
    \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Drawbacks of traditional methods}
      \begin{itemize}
      \item The detection performance for multiplicative faults is unqualified
      \item Unrecognized case where the variance becomes smaller
 	 \end{itemize}  
 	 \end{exampleblock}
\end{frame}

\begin{frame}{Motivation and solution}
\begin{exampleblock}{Motivation}
      \begin{itemize}
      \item Eliminate the shortcomings of traditional methods
      \item Enhance the fault detection performance
 	 \end{itemize}  
 	 \end{exampleblock}
 \begin{exampleblock}{Solution}
      \begin{itemize}
      \item $T^2$-statistic and Riemannian metric
      \item Support Vector Machine
 	 \end{itemize}  
 	 \end{exampleblock}

\end{frame}
\section{Calculation of monitoring index}
\begin{frame}{Data}
The data set $\textbf{y}$ is given, which has $n_1$ normal data and $n_2$
faulty data
\begin{equation}
   y = [y_1 \ y_2 \ \dots y_{n_1} \ y_{n_1+1} \ y_{n_1+2} \ \dots y_{n_1+n_2}] 
\end{equation}
Where $n_1 + n_2 = N$
\begin{equation}
    y(i) = 
    \begin{pmatrix}
        y_1(i) \\
        y_2(i) \\
       \vdots  \\
        y_m(i) \\ 
    \end{pmatrix}
\end{equation}
Where $\textbf{m}$ is the dimension of the output data. It is also called  degrees of freedom. % it is also called "degrees of freedom" in $T^2$ distribution.
\end{frame}
\begin{frame}{$T^2$-statistic}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            the steps of $T^2$-statistic:
      \begin{itemize}
      \item \textcolor{blue}{S1}: computing the mean
      \item \textcolor{blue}{S2}: computing the variance
      \item \textcolor{blue}{S3}: normalizing the data
      \item \textcolor{blue}{S4}: computing the  covariance
      \item \textcolor{blue}{S5}: computing the value of evaluation function
      \item \textcolor{blue}{S6}: Monitoring index
 	 \end{itemize}  
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
      \begin{equation} \nonumber
               \begin{aligned}
                    E(y) \approx \frac{1}{n_1}\sum_{i=1}^{n_1}y(i)   \\
                   \sigma^2 \approx \frac{1}{n_1-1}\sum_{i=1}^{n_1}y^2(i) \\
                    Y = \sigma^{-1}(y-E(y)) \\
                   \Sigma_y \approx \frac{YY^T}{N-1} \\
                   J_{T^2}(i) = y_i^T\Sigma_y^{-1}y_i \\
             \end{aligned}
     \end{equation}
      \begin{equation} 
                  J_{T^2} =  \begin{bmatrix}
                          J_{T^2}(1) \\
                          J_{T^2}(2) \\
                          \vdots      \\
                           J_{T^2}(N)
                         \end{bmatrix}
                         \label{Jt}
                \end{equation}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{$T^2$-statistic}
Where i = 1,2 \dots N. 
  $
                \sigma^{-1} \approx
                \begin{bmatrix}
                \sigma_1^{-1} &&&0\\
                &  \sigma_2^{-1} && \\
                && \ddots & \\
                0&&& \sigma_m{-1}
                \end{bmatrix}
  $ 
  \par
  In machine learning, this process can be called feature extraction.
  Feature:$ J_{T^2}(i)$
\end{frame}
\begin{frame}{Riemannian metric}
    \metroset{block=fill}
    \begin{exampleblock}{Common Riemannian manifolds}
	\begin{itemize}
    \item \textcolor{red}{symmetric positive definite (SPD) matrix manifolds}
    \item Grassmann manifolds
    \item Stiefel manifolds
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Geometry of SPD Matrices}
An n $\times$ n square matrix M is SPD, if and only if
\begin{itemize}
    \item $M^T = M$
    \item $u^TMu>0$ and $\forall u \neq 0$
\end{itemize}
SPD matrices have the following properties:
\begin{itemize}
\item 1. $\forall$ M $\in$ SPD(n), $M^{-1}$ $\in$ SPD(n) i.e., SPD matrices are invertible. 
\item 2. $\forall$ M $\in$ SPD(n), eigenvalues are positive i.e., Î»(M) > 0.
\end{itemize}
\end{frame}
\begin{frame}{Riemannian metric}
    \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{equation} \nonumber
          \begin{aligned}
          &S_i = C^{\frac{1}{2}}logm(C^{-\frac{1}{2}}C_iC^{-\frac{1}{2}})C^{\frac{1}{2}} \\
          &C_i = C^{\frac{1}{2}}expm(C^{-\frac{1}{2}}S_iC^{-\frac{1}{2}})C^{\frac{1}{2}} \\
          \end{aligned}
           \end{equation}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \includegraphics[width=0.7\textwidth]{RieDis.png}
                \caption{\href{https://www.mdpi.com/1424-8220/19/2/379}{Riemannian mainfold}} 
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Riemannian distance}
    \begin{equation}
R_d(C_1,C_2) = \norm{ C_1^{-\frac{1}{2}}C_2C_1^{-\frac{1}{2}}}_F = \left(  \sum_{i=1}^mlog^2\lambda_i  \right)^\frac{1}{2}
    \end{equation} 
    where $\norm{.}_F$ is the Frobenius norm, and $\lambda_i$'s are the eigenvalues  of $ C_1^{-\frac{1}{2}}C_2C_1^{-\frac{1}{2}}$
    \begin{equation}
        R_d(A^TC_1A,A^TC_2A)=R_d(C_1,C2)
    \end{equation}
    This is a very important property
\end{frame}
\begin{frame}{Riemannian distance as monitoring index}
   \begin{columns}
        \begin{column}{0.5\textwidth}
            the steps of Riemannian distance:
      \begin{itemize}
      \item \textcolor{blue}{S1}:   SPD matrix
      \item \textcolor{blue}{S2}:  Karcher mean
      \item \textcolor{blue}{S3}:  Riemannian distance
      \item  \textcolor{blue}{S4}:  Monitoring index
 	 \end{itemize}  
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
               \begin{equation}
              S_i = y(i)y(i)^T
               \end{equation}
                \begin{equation}
                  J_R(S_i,X) =  \left(  \sum_{i=1}^mlog^2\lambda_i  \right)^\frac{1}{2}
                \end{equation}
                 \begin{equation}
                  J_R =  \begin{bmatrix}
                          J_R(S_1,X) \\
                          J_R(S_2,X) \\
                          \vdots     \\
                           J_R(S_N,X)
                         \end{bmatrix}
                         \label{Jr}
                \end{equation}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Support Vector Machine}
\begin{columns}
        \begin{column}{0.5\textwidth}
           \begin{equation} \nonumber
           \begin{aligned}
               x = \begin{bmatrix}
                   J_{T^2}^T \\
                   J_R^T \\
                   \end{bmatrix} \\
                   w = \begin{bmatrix}
                        w_1 \\
                        w_2 \\
                       \end{bmatrix}
           \end{aligned}
           \end{equation}
           Task: find (w,b) to maximize $\frac{2}{\norm{w}}$
           \begin{equation} \nonumber
           D = sgn(w^Tx_i+b) = 
           \left\{  \begin{aligned} -1 \quad f = 0  \\
                                     1 \quad f \neq 0  \\   
                    \end{aligned}   
           \right.
           \end{equation}
           where D is the dicision function,$x_i:=x(:,i)$.
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
 % \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
 % \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
 % \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
 % \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Support Vector Machine}
    \begin{equation}
  \begin{aligned}
  &\gamma_i =\frac{|\hat{\gamma}_i|}{\sqrt{w^Tw}} = \frac{|w^Tx_i+b|}{\sqrt{w^Tw}} \\
  &\gamma_{\min}=\underset{i=1,\dots,N}{\min}\gamma_i
  \end{aligned}
  \end{equation}
    \begin{equation}
  \begin{aligned}
&  \underset{w,b}{\arg \max} \quad \gamma_{\min}=\underset{w,b}{\arg \max}\quad \underset{i=1,\dots,N}{\min}\gamma_i \\
& s.t. \quad y_i\left( \frac{w^Tx_i+b}{\sqrt{w^Tw}} \right) \geq \gamma_{\min}
  \end{aligned}
\end{equation}
    \begin{equation}
     \begin{aligned}
     & \underset{w,b}{\arg \min}\frac{1}{2}w^Tw \\
     & s.t. y_i(w^Tx_i+b) \geq 1, i = 1,\dots,N.
     \end{aligned} \label{svmcons}
 \end{equation}  
\end{frame}
\begin{frame}{Support Vector Machine}
 \begin{columns}
 \begin{column}{0.5\textwidth}
  \begin{equation} \nonumber
     \begin{aligned}
     &\min\limits_{w,b,\xi}\frac{1}{2}{\parallel \omega  \parallel}^2 
     + C \sum\limits_{i=1}^{N}\xi_i
     \\
     &s.t. y_i(w^Tx_i+b)\geq 1 - \xi_i, & i = 1,2,\dots,N 
     \\
     &\xi_i \geq 0  & i= 1,2,\dots,N
     \end{aligned}
     \label{noSeparable}
 \end{equation}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
  \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
  \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
  \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
  \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}



\section{The algorithm to find the threshold}


\begin{frame}{SMO algorithm}
The SMO(Sequential Minimal optimization) algorithm can find a set of $\alpha$.
Through the set of $ \alpha $, we can calculate the weights \textbf{w} and \textbf{b}.
\begin{equation}
    \begin{aligned}
   & \underset{\alpha}{\min}&&L(\alpha)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}
    \alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{N}\alpha_i \\
   & s.t. &&\sum_{i=1}^{N}\alpha_iy_i=0 \\
   & && 0 \leq\alpha_i\leq C 
    \end{aligned}
    \label{Cons_cond}
\end{equation}
\end{frame}
\begin{frame}{SMO algorithm}
Assume two variables $\alpha_1,\alpha_2$ are determined, rewrite the equation \ref{Cons_cond}:
\begin{equation}
    \underset{\alpha_1,\alpha_2}{\min}  L(\alpha)= \underset{\alpha_1,\alpha_2}{\min}W(\alpha_1,\alpha_2)
    +\frac{1}{2}\sum_{i=3}^{N}\sum_{j=3}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=3}^{N}\alpha_i
\end{equation}
\begin{equation}
    \begin{aligned}
    \underset{\alpha_1,\alpha_2}{\min}W(\alpha_1,\alpha_2)
   &=&&\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2-(\alpha_1+\alpha_2) \\
   & &&+y_1\alpha_1\sum_{i=3}^{N}y_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2} \\
   &s.t.&&\alpha_1y_1+\alpha_2y_2 = -\sum_{i=3}^Ny_i\alpha_i = \zeta
    \end{aligned}
    \label{SMO_cond}
\end{equation}
where $\zeta$ is constant.
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
 \begin{columns}
             \begin{column}{0.5\textwidth}
               SMO algorithm  $\rightarrow$ (w,b) \par
The hyperplane: \par
            $
             w^Tx+b = 0
            $  \par
 The decision function: \par
$
  \begin{aligned}
  & f(x) =  
 w^Tx_i+b  \\
  & D = \left\{
     \begin{aligned}
      &-1\quad &f(x)<0   \\
      &\quad 1    \quad &f(x)>0 
     \end{aligned}
  \right. 
    \end{aligned}
$
            
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
  \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
  \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
  \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
  \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
Assume $D = -1$ is fault-free, $FAR = \alpha$,$FDR =\beta$.\par
The (practical) FD specific requirements: \par
$Pr(D=1|f=0)\leq \alpha$ or\par
$Pr(D=1|f \neq 0)\geq \beta$ \par
If the requirements are not meet: \par
$Pr(D=1|f=0)> \alpha$ or\par
$Pr(D=1|f \neq 0) < \beta$ \par
how to solve the problem?
\end{frame}

\begin{frame}{Tune hyperplane based on FAR or FDR}
$Pr(D=1|f=0) \approx \frac{1}{n_1}\sum_{i=1}^{n_1}I_{i-}$  \par
$Pr(D=1|f \neq 0) \approx          \frac{1}{n_2}\sum_{j=1}^{n_2}I_{j+}$ \par
\begin{equation}
I_{i-} = \left\{ \begin{aligned}
0 \quad  w^Tx_{n_i} + b < 0 \\
1 \quad w^Tx_{n_i} +b  > 0 
\end{aligned}
\right.
\end{equation}
where $x_n$ is fault-free data, $i=1,2,\dots,n_1$
\begin{equation}
I_{i+} = \left\{ \begin{aligned}
0 \quad  w^Tx_{f_j} + b < 0 \\
1 \quad w^Tx_{f_j} +b  > 0 
\end{aligned}
\right.
\end{equation}
where $x_f$ is faulty data, $j=1,2,\dots,n_2$
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
Idea: Change b to meet FAR or FDR requirements. \par
Case 1: $Pr(D=1|f=0) > \alpha$
\begin{exampleblock}{Steps}
      \begin{enumerate}
      \item Giving a small step length $\epsilon > 0$
      \item $b = b - \epsilon$, updating the decision function
      \item Computing  $Pr(D=1|f=0)$
      \item If  $Pr(D=1|f=0) > \alpha$, then go to the step 2 
 	 \end{enumerate}  
 	 \end{exampleblock}
\end{frame}

\begin{frame}{Tune hyperplane based on FAR or FDR}
    Case 2: $Pr(D=1|f\neq 0) < \beta$
\begin{exampleblock}{Steps}
      \begin{enumerate}
      \item Giving a small step length $\epsilon > 0$
      \item $b = b + \epsilon$, updating the decision function
      \item Computing  $Pr(D=1|f\neq 0)$
      \item If  $Pr(D=1|f\neq 0) < \beta$, then go to the step 2 
 	 \end{enumerate}  
 	 \end{exampleblock}
\end{frame}
\section{Benchmark study}
\begin{frame}{Generating the data}
    $y = y + f, \left\{ \begin{aligned}
    f = 0      \quad &\text{fault-free} \\
    f \neq 0   \quad &\text{additive faults}
     \end{aligned} 
     \right. $
   $y = My,\left\{ \begin{aligned}
    M = Id     \quad &\text{fault-free} \\
    f \neq Id   \quad &\text{multiplicative faults}
     \end{aligned} 
     \right. $
     \metroset{block=fill}
      \begin{exampleblock}{Relation}
	\begin{itemize}
    \item Changes in mean can be mapped to additive faults
    \item Changes in variance can be mapped to multiplicative faults
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Generating the data}
\begin{center}
\begin{tabular}{ccc}
\hline
y& number& Distribution                                    \\
\hline
$y_n  $ &$n_1$& $y_n \sim N(\mu, \sigma^2)$                \\
$y_a  $ &$n_2$& $y_{a_i} \sim N(\mu_i,\sigma^2$)           \\
$y_m  $ &$n_3$& $y_{m_j} \sim N(\mu,\sigma_j^2)$           \\
$y_{am}$&$n_4$& $y_{am_k}\sim N(\mu_k,\sigma_k^2)$         \\
$y_{ms}$ &$n_5$& $y_{ms_l} \sim N(\mu,\sigma_l^2)$  \\
\hline
\end{tabular}
\end{center}
where $\mu \neq \mu_i,\mu \neq \mu_j,\mu \neq \mu_k,\sigma \neq \sigma_i,\sigma < \sigma_j,\sigma \neq \sigma_k,\sigma > \sigma_l$. \par Every \textbf{p} samples as a group
$y_{a_i},y_{m_i},y_{am_i}$. \par $i = 1,2,\dots,\frac{n_2}{p},j=1,2,\dots,\frac{n_3}{p}$ \par
$k = 1,2,\dots,\frac{n_4}{p},l = 1,2,\dots, \frac{n_5}{l}$
\end{frame}
\begin{frame}{Case study on a numerical example}
      \begin{figure}
        \centering
        \includegraphics[width=8cm]{fig/Coriginal.eps}
        \caption{fault-free and faulty data}
    \end{figure}
\end{frame}
\begin{frame}{Case study on a numerical example}
    \begin{columns}
             \begin{column}{0.5\textwidth}
 \begin{small}
\begin{equation} \nonumber
\begin{aligned}
& w^Tx+b=0  \\
& D=sgn(w^Tx_i+b) = \left\{ 
    \begin{aligned}
  & -1  \\
  &\quad 1 
  \end{aligned}
     \right.
\end{aligned}
\end{equation}
\end{small}
For case 1: \par  $\small{w^T = [ 0.4106 \quad -0.6689]
 , b = -3.473} $\par $\small{\alpha \approx 0.047\%,\beta  \approx  99.9\%,VA \approx 99.93\%}$ \par 
For case 2:  \par $\small{w^T = [0.3305 \quad -0.7488]
 , b = -0.8336} $\par $\small{\alpha \approx 0.33\%  ,\beta  \approx  98.8 \%,VA = \approx 89.83 \%}$ \par
 where VA is validation accuracy.
           \end{column}
        \begin{column}{0.5\textwidth} 
         \begin{figure}
        \centering
        \includegraphics[width=4cm]{fig/addHyper.eps}
        \caption{$y_n$ and $y_a$ data(case 1)}
        \includegraphics[width=4cm]{fig/ALhyperplane.eps}
        \caption{$y_n$ and $y_{am}$ data(case 2)}
        \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Case study on a numerical example}
 \begin{columns}
             \begin{column}{0.5\textwidth}
    For case 3: \par
     $\small{w^T = [0.2859 \quad -0.1169]
 , b =-1.7315 } $\par $\small{\alpha  \approx 4.7\%,\beta  \approx 95 \% ,VA \approx}90.11 \% $ \par 
 With reference to the values of $\alpha$ and $\beta$, the prediction accuracy is satisfactory. But compared to case 1 and case 2, it has the worst performance.
           \end{column}
        \begin{column}{0.4\textwidth}  %%<--- here
   \begin{figure}
        \centering
        \includegraphics[width=4cm]{fig/EAandhyper.eps}
        \caption{$y_n$ data(case 3)}
        \includegraphics[width=4cm]{fig/EMandhyper.eps}
        \caption{ $y_{m}$ data(case 3)}
        \end{figure}
        \end{column}
    \end{columns}    
\end{frame}

\begin{frame}{Case study on a numerical example}
     \metroset{block=fill}
      \begin{exampleblock}{Data reoptimization}
	\begin{itemize}
    \item Cumulative  $T^2$-statistic
    \item Cumulative  Riemannian Metric
    \end{itemize}
    \end{exampleblock}
    \begin{small}
    \begin{equation}\nonumber
       \begin{aligned}
       J_{T_n^2}(j) = \frac{1}{n}\sum_{i}^{i+n-1}J_{T^2}(i) \\
       J_{R_n}(j) = \frac{1}{n}\sum_{i}^{i+n-1}J_R(i)
       \end{aligned}
    \end{equation}
    \end{small}
\end{frame}
\begin{frame}{Case study on a numerical example}
     \begin{columns}
             \begin{column}{0.5\textwidth}
       For case 4: \par
     $\scriptsize{w^T = [ 0.8847 \quad 0.0007]
 , b =-6.3180 } $\par $\scriptsize{\alpha  \approx 0\%,\beta  \approx 99.99 \% ,VA \approx 99.99} \% $ \par 
 For case 5: \par 
   $\scriptsize{w^T = [0.0481 \quad 5.2780]
 , b = -23.5837} $\par $\scriptsize{\alpha  \approx 3.3333\times 10^{-3} \%,\beta  \approx 99.97\% }$ \par 
$VA \approx 99.99 \% $
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
    \begin{figure}
        \centering
        \includegraphics[width=4cm]{fig/Cu3.eps}
        \caption{$y_n,y_a,y_m,y_{am}$ (case 4)}
        \includegraphics[width=4cm]{fig/miniVarP.eps}
        \caption{ $y_n,y_a,y_{m},y_{am},y_{ms}$ (case 5)}
        \end{figure}
  
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Case study on a numerical example}
     \begin{columns}
             \begin{column}{0.5\textwidth}
              \begin{figure}
              \centering
           \includegraphics[width=5cm]{fig/perfectR.eps}
            \end{figure}
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
   \begin{figure}
        \centering
        \includegraphics[width=3cm]{fig/Coriginal.eps} \\
        \includegraphics[width=3cm]{fig/thresholdT.eps} \\
        \includegraphics[width=3cm]{fig/miniVarP.eps}
        \end{figure}
  
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Case study on three-tank system(TTS)}
    \begin{columns}
             \begin{column}{0.5\textwidth}
        $w^T = [0.0481 \quad 5.2780]$ $b = -23.5837$
    \begin{equation} \nonumber
  \begin{aligned}
   & FAR \approx \frac{14}{14+882331} \approx 1.5867\times 10^{-3} \% \\
   & FDR \approx 1 - \frac{48651}{48651+933651} \approx 95.05 \% \\
   & VA \approx 1 - \frac{48651+14}{933651+882331} \approx 97.32 \% 
  \end{aligned}
\end{equation}
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
   \begin{figure}
        \centering
        \includegraphics[width=4cm]{fig/realori.eps}
        \caption{real data from TTS}
        \includegraphics[width=4cm]{fig/realdata.eps}
        \caption{The hyperplane of the data}
        \end{figure}
  
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Case study on continuous stirred tank heater(CSTH)}
     \begin{columns}
             \begin{column}{0.5\textwidth}
 $w^T = [0.1775 \quad 2.2058]$,$b = -7.1464$
\begin{equation} \nonumber
  \begin{aligned}
   & FAR \approx \frac{1}{1+1142} \approx  8.7489\times 10^{-2} \% \\
   & FDR \approx 1 - \frac{0}{0+2743} \approx 100 \% \\
   & VA \approx 1 - \frac{1+0}{1143+2743} \approx 99.97 \% 
  \end{aligned}
\end{equation}
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
   \begin{figure}
        \centering
        \includegraphics[width=4cm]{fig/csthrealdata.eps}
        \caption{real data from CSTH}
        \includegraphics[width=4cm]{fig/Csthboundary.eps}
        \caption{The hyperplane of the data}
        \end{figure}
  
        \end{column}
    \end{columns}
\end{frame}
\section{Conclusion and future work}

\begin{frame}{Conclusion}
 \metroset{block=fill}
    \begin{exampleblock}{Advantage}
	\begin{itemize}
    \item Different faults are distributed in different areas
    \item A satisfactory fault detection performance(sufficient data)
    \item Ability to detect when the variance becomes smaller
    \end{itemize}
    \end{exampleblock}
    
     \metroset{block=fill}
    \begin{exampleblock}{Disadvantage}
	\begin{itemize}
    \item Need a lot of data
    \item Offline training requires a lot of calculations
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Future work}
      \begin{itemize}
      \item \textcolor{blue}{1}: Combine 3 FD methods using SVM
        \item \textcolor{blue}{2}: determine the decision function by the degree of freedom and the variance of the data
      \item \textcolor{blue}{3}: Random algorithms to find the threshold of two-dimensional data or the threshold of three-dimensional data
 	 \end{itemize}  
\end{frame}

\begin{frame}[standout]
\begin{center}
Thank you for your attention!
\end{center}
\end{frame}

\section{Appendix}
\end{document}
