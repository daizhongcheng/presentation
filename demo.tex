\documentclass[10pt]{beamer}
\usepackage{amsmath}
\usepackage{mathdots}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetheme[progressbar=frametitle]{metropolis}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{pgfplots}
\usepackage{amsmath}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepgfplotslibrary{dateplot}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{A Fault Detection method based on SVM technique with $T^2$-Statistic and Riemannian metric}
%\subtitle{going beyond IF and Scopus index (v2.0)}
\author{Zhongcheng Dai  \\
 Supervisor: \\
 Prof. Dr. Ing. Steven Ding\\
Dr. Ing. Chris Louen 
}
 \date{30 November 2019}
\institute{Automatic Control and Complex Systems}
\titlegraphic{\hfill\includegraphics[height=1cm]{logo_ITB}}
\begin{document}
\maketitle
\begin{frame}{contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}
\section{Introduction}
\begin{frame}{Background}
\metroset{block=fill}
    \begin{exampleblock}{Traditional methods}
	\begin{itemize}
    \item $T^2$-statistic
    \item Q-statistic
    \item Kullback-Leibler divergence
    \item The Wishart distribution-based methods
    \item \dots \dots
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Background}
 A brief summary of the steps:
      \begin{itemize}
      \item \textcolor{blue}{S1}: collect sufficient fault-free data
      \item \textcolor{blue}{S2}: offline train to determine the threshold
      \item \textcolor{blue}{S3}: use the threshold to check the new data
 	 \end{itemize}  
\end{frame}
\begin{frame}{Background}
 \begin{exampleblock}{merits of the traditional methods}
	\begin{itemize}
    \item The detection performance for additive faults is practical
    \item Online calculations are small, only need to compare threshold
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Background}
\begin{exampleblock}{Drawbacks of traditional methods}
      \begin{itemize}
      \item The detection performance for multiplicative faults is unqualified
      \item Unrecognized case where the variance becomes smaller
 	 \end{itemize}  
 	 \end{exampleblock}
\end{frame}
\begin{frame}{Motivation and solution}
\begin{exampleblock}{Motivation}
      \begin{itemize}
      \item Eliminate the shortcomings of traditional methods
      \item Enhance the fault detection performance
 	 \end{itemize}  
 	 \end{exampleblock}
 \begin{exampleblock}{Solution}
      \begin{itemize}
      \item $T^2$-statistic and Riemannian metric
      \item Support Vector Machine
 	 \end{itemize}  
 	 \end{exampleblock}

\end{frame}
\section{Basic knowledge}
\begin{frame}{Data}
The data set $\textbf{y}$ is given, which has $n_1$ normal data and $n_2$
faulty data
\begin{equation}
   y = [y_1 \ y_2 \ \dots y_{n_1} \ y_{n_1+1} \ y_{n_1+2} \ \dots y_{n_1+n_2}] 
\end{equation}
Where $n_1 + n_2 = N$
\begin{equation}
    y(i) = 
    \begin{pmatrix}
        y_1(i) \\
        y_2(i) \\
       \vdots  \\
        y_m(i) \\ 
    \end{pmatrix}
\end{equation}
Where $\textbf{m}$ is the dimension of the output data. % it is also called "degrees of freedom" in $T^2$ distribution.
\end{frame}
\begin{frame}{$T^2$-statistic}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            the steps of $T^2$-statistic:
      \begin{itemize}
      \item \textcolor{blue}{S1}: computing the mean
      \item \textcolor{blue}{S2}: computing the variance
      \item \textcolor{blue}{S3}: Normalizing the data
      \item \textcolor{blue}{S4}: computing the  covariance
      \item \textcolor{blue}{S5}: computing the value of evaluation function
 	 \end{itemize}  
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
               \begin{equation}
               E(y) \approx \frac{1}{n_1}\sum_{i=1}^{n_1}y(i) 
               \end{equation}
                \begin{equation}
                   \sigma^2 \approx \frac{1}{n_1-1}\sum_{i=1}^{n_1}y^2(i) 
                \end{equation}
                \begin{equation}
                Y = \sigma^{-1}(y-E(y))
                \end{equation}
                \begin{equation}
                \Sigma_y \approx \frac{YY^T}{N-1}
                \end{equation}
                \begin{equation}
                       J_{T^2}(i) = y_i^T\Sigma_y^{-1}y_i
                \end{equation}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{$T^2$-statistic}
Where i = 1,2 \dots N. 
  $
                \sigma^{-1} \approx
                \begin{bmatrix}
                \sigma_1^{-1} &&&0\\
                &  \sigma_2^{-1} && \\
                && \ddots & \\
                0&&& \sigma_m{-1}
                \end{bmatrix}
  $ 
  \par
  In machine learning, this process can be called feature extraction.
  Feature:$ J_{T^2}(i)$
\end{frame}
\begin{frame}{Riemannian metric}
    \metroset{block=fill}
    \begin{exampleblock}{Common Riemannian manifolds}
	\begin{itemize}
    \item \textcolor{red}{symmetric positive definite (SPD) matrix manifolds}
    \item Grassmann manifolds
    \item Stiefel manifolds
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Geometry of SPD Matrices}
An n $\times$ n square matrix M is SPD matrix, if and only if
\begin{itemize}
    \item $M^T = M$
    \item $u^TMu>0$ and $\forall u \neq 0$
\end{itemize}
SPD matrices have the following properties:
\begin{itemize}
\item 1. $\forall$ M $\in$ SPD(n), $M^{-1}$ $\in$ SPD(n) i.e., SPD matrices are invertible. 
\item 2. $\forall$ M $\in$ SPD(n), eigenvalues are positive i.e., Î»(M) > 0.
\end{itemize}
\end{frame}
\begin{frame}{Riemannian metric}
    \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{equation} \nonumber
          \begin{aligned}
          &S_i = C^{\frac{1}{2}}logm(C^{-\frac{1}{2}}C_iC^{-\frac{1}{2}})C^{\frac{1}{2}} \\
          &C_i = C^{\frac{1}{2}}expm(C^{-\frac{1}{2}}S_iC^{-\frac{1}{2}})C^{\frac{1}{2}} \\
          \end{aligned}
           \end{equation}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \includegraphics[width=0.7\textwidth]{RieDis.png}
                \caption{\href{https://www.mdpi.com/1424-8220/19/2/379}{Riemannian mainfold}} 
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Riemannian distance}
    \begin{equation}
R_d(C_1,C_2) = \norm{ C_1^{-\frac{1}{2}}C_2C_1^{-\frac{1}{2}}}_F = \left(  \sum_{i=1}^mlog^2\lambda_i  \right)^\frac{1}{2}
    \end{equation} 
    where $\norm{.}_F$ is the Frobenius norm, and $\lambda_i$'s are the eigenvalues  of $ C_1^{-\frac{1}{2}}C_2C_1^{-\frac{1}{2}}$
    \begin{equation}
        R_d(A^TC_1A,A^TC_2A)=R_d(C_1,C2)
    \end{equation}
    This is a very important property
\end{frame}
\begin{frame}{Riemannian distance}
   \begin{columns}
        \begin{column}{0.5\textwidth}
            the steps of Riemannian distance:
      \begin{itemize}
      \item \textcolor{blue}{S1}:   SPD matrix
      \item \textcolor{blue}{S2}:  Karcher mean
      \item \textcolor{blue}{S3}:  Riemannian distance
 	 \end{itemize}  
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
               \begin{equation}
              S_i = y(i)y(i)^T
               \end{equation}
                \begin{equation}
                  J_R(S_i,X) =  \left(  \sum_{i=1}^mlog^2\lambda_i  \right)^\frac{1}{2}
                \end{equation}
                
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Support Vector Machine}
\begin{columns}
        \begin{column}{0.5\textwidth}
           \begin{equation} \nonumber
           \begin{aligned}
               x = \begin{bmatrix}
                   J_{T^2} \\
                   J_R   \\
                   \end{bmatrix} \\
                   w = \begin{bmatrix}
                        w_1 \\
                        w_2 \\
                       \end{bmatrix}
           \end{aligned}
           \end{equation}
           Task: find (w,b) to maximize $\frac{2}{\norm{w}}$
           \begin{equation} \nonumber
           D = sgn(w^Tx+b) = 
           \left\{  \begin{aligned} -1 \quad f = 0  \\
                                     1 \quad f \neq 0  \\   
                    \end{aligned}   
           \right.
           \end{equation}
           D is the dicision function.
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
 % \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
 % \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
 % \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
 % \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Support Vector Machine}
    \begin{equation}
  \begin{aligned}
  &\gamma_i =\frac{|\hat{\gamma}_i|}{\sqrt{w^Tw}} = \frac{|w^Tx_i+b|}{\sqrt{w^Tw}} \\
  &\gamma_{\min}=\underset{i=1,\dots,N}{\min}\gamma_i
  \end{aligned}
  \end{equation}
    \begin{equation}
  \begin{aligned}
&  \underset{w,b}{\arg \max} \quad \gamma_{\min}=\underset{w,b}{\arg \max}\quad \underset{i=1,\dots,N}{\min}\gamma_i \\
& s.t. \quad y_i\left( \frac{w^Tx_i+b}{\sqrt{w^Tw}} \right) \geq \gamma_{\min}
  \end{aligned}
\end{equation}
    \begin{equation}
     \begin{aligned}
     & \underset{w,b}{\arg \min}\frac{1}{2}w^Tw \\
     & s.t. y_i(w^Tx_i+b) \geq 1, i = 1,\dots,N.
     \end{aligned} \label{svmcons}
 \end{equation}  
\end{frame}
\begin{frame}{Support Vector Machine}
 \begin{columns}
 \begin{column}{0.5\textwidth}
  \begin{equation} \nonumber
     \begin{aligned}
     &\min\limits_{w,b,\xi}\frac{1}{2}{\parallel \omega  \parallel}^2 
     + C \sum\limits_{i=1}^{N}\xi_i
     \\
     &s.t. y_i(w^Tx_i+b)\geq 1 - \xi_i, & i = 1,2,\dots,N 
     \\
     &\xi_i \geq 0  & i= 1,2,\dots,N
     \end{aligned}
     \label{noSeparable}
 \end{equation}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
  \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
  \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
  \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
  \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}



\section{The Algorithm to find the the threshold}


\begin{frame}{SMO algorithm}
The SMO(Sequential Minimal optimization) algorithm can find a set of $\alpha$.
Through the set of $ \alpha $, we can calculate the weights \textbf{w} and \textbf{b}.
\begin{equation}
    \begin{aligned}
   & \underset{\alpha}{\min}&&L(\alpha)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}
    \alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{N}\alpha_i \\
   & s.t. &&\sum_{i=1}^{N}\alpha_iy_i=0 \\
   & && 0 \leq\alpha_i\leq C 
    \end{aligned}
    \label{Cons_cond}
\end{equation}
\end{frame}
\begin{frame}{SMO algorithm}
Assume two variables $\alpha_1,\alpha_2$ are determined, rewrite the equation \ref{Cons_cond}:
\begin{equation}
    \underset{\alpha_1,\alpha_2}{\min}  L(\alpha)= \underset{\alpha_1,\alpha_2}{\min}W(\alpha_1,\alpha_2)
    +\frac{1}{2}\sum_{i=3}^{N}\sum_{j=3}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=3}^{N}\alpha_i
\end{equation}
\begin{equation}
    \begin{aligned}
    \underset{\alpha_1,\alpha_2}{\min}W(\alpha_1,\alpha_2)
   &=&&\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2-(\alpha_1+\alpha_2) \\
   & &&+y_1\alpha_1\sum_{i=3}^{N}y_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2} \\
   &s.t.&&\alpha_1y_1+\alpha_2y_2 = -\sum_{i=3}^Ny_i\alpha_i = \zeta
    \end{aligned}
    \label{SMO_cond}
\end{equation}
where $\zeta$ is constant.
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
 \begin{columns}
             \begin{column}{0.5\textwidth}
               SMO algorithm  $\rightarrow$ (w,b) \par
The hyperplane: \par
            $
             w^Tx+b = 0
            $  \par
 The decision function: \par
$
  \begin{aligned}
  & f(x) =  
 w^Tx_i+b  \\
  & D = \left\{
     \begin{aligned}
      &-1\quad &f(x)<0   \\
      &\quad 1    \quad &f(x)>0 
     \end{aligned}
  \right. 
    \end{aligned}
$
            
           \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}[!htb] 
                \centering 
                \begin{tikzpicture}[scale = 0.7]
  % Draw axes
  \draw [thick] (0,5) node (yaxis) [above] {$J_R$}
        |-(5,0) node (xaxis) [right] {$J_{T^2}$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (1,-2) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\mathbf{w}^T \mathbf{x} + b = -1$};
  % draw distance
  \draw[dotted] (4,5) -- (6,3);
  \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
  \draw[dotted] (0,0) -- (0.5,-0.5);
  \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
  \draw [->] (2,1) -- (1.5,1.5);
  \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
  % draw negative dots
  \fill[red] (0.5,1.5) circle (3pt);
  \fill[red]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
  \fill[black] (0.5,1)     circle (3pt);  % the outlier of the data
  \fill[black] (2,0.5)     circle (3pt);
  % draw positive dots
  \draw[red,thick] (4,1)     circle (3pt); 
  \draw[red,thick] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
  \draw[black]     (3,1.5)   circle (3pt); % the outlier of the data
  \draw[black]     (2,2)     circle (3pt);
\end{tikzpicture}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
Assume $D = -1$ is fault-free, $FAR = \alpha$,$FDR =\beta$.\par
The (practical) FD specific requirements: \par
$Pr(D=1|f=0)\leq \alpha$ or\par
$Pr(D=1|f \neq 0)\geq \beta$ \par
If the requirements are not meet: \par
$Pr(D=1|f=0)> \alpha$ or\par
$Pr(D=1|f \neq 0) < \beta$ \par
how to solve the problem?
\end{frame}

\begin{frame}{Tune hyperplane based on FAR or FDR}
$Pr(D=1|f=0) \approx \frac{1}{n_1}\sum_{i=1}^{n_1}I_{i-}$  \par
$Pr(D=1|f \neq 0) \approx          \frac{1}{n_2}\sum_{j=1}^{n_2}I_{j+}$ \par
\begin{equation}
I_{i-} = \left\{ \begin{aligned}
0 \quad  w^Tx_{n_i} + b < 0 \\
1 \quad w^Tx_{n_i} +b  > 0 
\end{aligned}
\right.
\end{equation}
where $x_n$ is fault-free data, $i=1,2,\dots,n_1$
\begin{equation}
I_{i+} = \left\{ \begin{aligned}
0 \quad  w^Tx_{f_j} + b < 0 \\
1 \quad w^Tx_{f_j} +b  > 0 
\end{aligned}
\right.
\end{equation}
where $x_f$ is faulty data, $j=1,2,\dots,n_2$
\end{frame}
\begin{frame}{Tune hyperplane based on FAR or FDR}
Idea: Change b to meet FAR or FDR requirements. \par
Case 1: $Pr(D=1|f=0) > \alpha$
\begin{exampleblock}{Steps}
      \begin{enumerate}
      \item Giving a small step length $\epsilon > 0$
      \item $b = b - \epsilon$, updating the decision function
      \item Computing  $Pr(D=1|f=0)$
      \item If  $Pr(D=1|f=0) > \alpha$, then go to the step 2 
 	 \end{enumerate}  
 	 \end{exampleblock}
\end{frame}

\begin{frame}{Tune hyperplane based on FAR or FDR}
    Case 2: $Pr(D=1|f\neq 0) < \beta$
\begin{exampleblock}{Steps}
      \begin{enumerate}
      \item Giving a small step length $\epsilon > 0$
      \item $b = b + \epsilon$, updating the decision function
      \item Computing  $Pr(D=1|f\neq 0)$
      \item If  $Pr(D=1|f\neq 0) < \beta$, then go to the step 2 
 	 \end{enumerate}  
 	 \end{exampleblock}
\end{frame}
\section{Benchmark study}
\begin{frame}{Generating the data}
    $y = y + f, \left\{ \begin{aligned}
    f = 0      \quad &fault-free \\
    f \neq 0   \quad &\text{additive faults}
     \end{aligned} 
     \right. $
   $y = My,\left\{ \begin{aligned}
    M = Id     \quad &fault-free \\
    f \neq Id   \quad &\text{multiplicative faults}
     \end{aligned} 
     \right. $
     \metroset{block=fill}
      \begin{exampleblock}{Relation}
	\begin{itemize}
    \item Changes in averages can be mapped to additive faults
    \item Changes in variance can be mapped to multiplicative faults
    \end{itemize}
    \end{exampleblock}
\end{frame}
\begin{frame}{Generating the data}
\begin{center}
\begin{tabular}{ccc}
\hline
y& number& Distribution                                    \\
\hline
$y_n  $ &$n_1$& $y_n \sim N(\mu, \sigma^2)$                \\
$y_a  $ &$n_2$& $y_{a_i} \sim N(\mu_i,\sigma^2$)           \\
$y_m  $ &$n_3$& $y_{m_j} \sim N(\mu,\sigma_j^2)$           \\
$y_{am}$&$n_4$& $y_{am_k}\sim N(\mu_k,\sigma_k^2)$         \\
$y_{ms}$ &$n_5$& $y_{ms_l} \sim N(\mu,\sigma_l^2)$  \\
\hline
\end{tabular}
\end{center}
where $\mu \neq \mu_i,\mu \neq \mu_j,\mu \neq \mu_k,\sigma \neq \sigma_i,\sigma < \sigma_j,\sigma \neq \sigma_k,\sigma > \sigma_l$. \par Every p samples as a group
$y_{a_i},y_{m_i},y_{am_i}$. \par $i = 1,2,\dots,\frac{n_2}{p},j=1,2,\dots,\frac{n_3}{p}$ \par
$k = 1,2,\dots,\frac{n_4}{p},l = 1,2,\dots, \frac{n_5}{l}$
\end{frame}

\section{Conclusions and future work}

\begin{frame}{take home notes}

\begin{center}science is about:\end{center}

\begin{center}
\begin{enumerate}
	\item \textcolor{blue}{honesty} in researching the problem
	\item \textcolor{blue}{bravery} in publishing the results 
    \item \textcolor{blue}{big heart} in getting feedback 
\end{enumerate}
\end{center}
\end{frame}

\begin{frame}[standout]
\begin{center}
Thank you for your attention!
\end{center}
\end{frame}
\end{document}
